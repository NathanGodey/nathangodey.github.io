---
title: 'MANTa: Efficient Gradient-Based Tokenization for End-to-End Robust Language Modeling'
date: 2023-06-09 15:00:00 +0200
categories: [featured, publications]
image: img/thumbnails/manta_thumb-min.jpg
description: "We train a gradient-based neural tokenizer which learns segmentation softly, and improves robustness to misspellings and specific domains with atypical vocabularies."
author: me
tags: [thesis, tokenization]     # TAG names should always be lowercase
---

Last year, I got my first paper published as a findings at EMNLP 2022! It was a joint effort with [Roman Castagn√©](https://romancast.github.io/) and was co-authored by my PhD supervisors [Eric Villemonte de la Clergerie](http://alpage.inria.fr/~clerger/) and [Beno√Æt Sagot](http://alpage.inria.fr/~sagot/) from Inria's ALMAnaCH team.

It introduces a differentiable tokenization that can be plugged to many language models to make end-to-end neural language modeling.

Here is the PDF version of the paper that [you can also find here](https://aclanthology.org/2022.findings-emnlp.207):
<iframe src="https://docs.google.com/gview?url=https://github.com/NathanGodey/nathangodey.github.io/raw/main/_includes/pdfs/manta.pdf&embedded=true" style="width:718px; height:700px;" frameborder="0"></iframe>

We also released our models' weights and implementation on HuggingFace ü§ó
- [Small MANTa-LM](https://huggingface.co/almanach/manta-lm-small)
- [Base MANTa-LM](https://huggingface.co/almanach/manta-lm-base)


Please cite as:
```bibtex
@inproceedings{godey-etal-2022-manta,
    title = "{MANT}a: Efficient Gradient-Based Tokenization for End-to-End Robust Language Modeling",
    author = "Godey, Nathan  and
      Castagn{\'e}, Roman  and
      de la Clergerie, {\'E}ric  and
      Sagot, Beno{\^\i}t",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.207",
    pages = "2859--2870",
}
```

*This work was funded by the PRAIRIE institute as part of a PhD contract at Inria Paris and Sorbonne Universit√©.*